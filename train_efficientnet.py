# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ Ä‘áº¹p vÃ  tÃ­nh toÃ¡n xá»‹n
#!pip install "protobuf==4.25.3"
#!pip install -q scikit-learn seaborn matplotlib

import matplotlib.pyplot as plt
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, losses, callbacks
from tensorflow.keras.applications import EfficientNetB0
from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

print(f"ğŸ”¥ TensorFlow Version: {tf.__version__}")
print("âœ… ÄÃ£ sáºµn sÃ ng!")

# ==========================================
# Cáº¤U HÃŒNH Há»† THá»NG
# ==========================================
DATA_PATH = "/kaggle/input/raw-images/input_images"

IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 25

# ==========================================
# HÃ€M ÄIá»€U CHá»ˆNH Tá»C Äá»˜ Há»ŒC (LR SCHEDULER)
# ==========================================
def lr_scheduler(epoch, lr):
    # 5 vÃ²ng Ä‘áº§u: Khá»Ÿi Ä‘á»™ng nháº¹ (Warmup)
    if epoch < 5:
        return lr + (0.001 - 1e-5) / 5
    # CÃ¡c vÃ²ng sau: Giáº£m dáº§n theo hÃ¬nh Sin (Cosine Decay)
    else:
        return 0.001 * 0.5 * (1 + np.cos(np.pi * (epoch - 5) / (EPOCHS - 5)))

print("âœ… ÄÃ£ thiáº¿t láº­p cáº¥u hÃ¬nh!")

# ==========================================
# LOAD Dá»® LIá»†U & TÃNH TOÃN CÃ‚N Báº°NG
# ==========================================
print("â³ Äang Ä‘á»c dá»¯ liá»‡u...")

# Load táº­p Train
train_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_PATH, validation_split=0.2, subset="training", seed=42,
    image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, label_mode='categorical'
)

# Load táº­p Validation
val_ds = tf.keras.utils.image_dataset_from_directory(
    DATA_PATH, validation_split=0.2, subset="validation", seed=42,
    image_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, label_mode='categorical'
)

class_names = train_ds.class_names
NUM_CLASSES = len(class_names)
print(f"ğŸ“¦ TÃ¬m tháº¥y {NUM_CLASSES} lá»›p: {class_names}")

# --- TÃNH TOÃN CLASS WEIGHTS ---
# (GiÃºp model chÃº Ã½ ká»¹ hÆ¡n vÃ o cÃ¡c chá»¯ cÃ¡i cÃ³ Ã­t áº£nh)
print("âš–ï¸ Äang tÃ­nh toÃ¡n trá»ng sá»‘ (Class Weights)... Äá»£i xÃ­u nhÃ©!")
y_train = []
# Duyá»‡t qua 1 vÃ²ng data Ä‘á»ƒ láº¥y nhÃ£n (Máº¥t khoáº£ng 1-2 phÃºt)
for _, labels in train_ds:
    y_train.extend(np.argmax(labels.numpy(), axis=1))

class_weights_vals = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train),
    y=y_train
)
class_weights_dict = dict(enumerate(class_weights_vals))
print("âœ… ÄÃ£ cÃ¢n báº±ng xong! Model sáº½ há»c cÃ´ng báº±ng hÆ¡n.")

# Tá»‘i Æ°u hÃ³a bá»™ nhá»› Ä‘á»‡m
AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

# ==========================================
# XÃ‚Y Dá»°NG MODEL (FINE-TUNING NGAY Tá»ª Äáº¦U)
# ==========================================
# 1. Augmentation máº¡nh máº½ (Giáº£ láº­p mÃ´i trÆ°á»ng xáº¥u)
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomContrast(0.2),
    layers.RandomBrightness(0.2),
    layers.RandomTranslation(0.1, 0.1),
    layers.GaussianNoise(0.1)
])

# 2. Táº£i EfficientNetB0
base_model = EfficientNetB0(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')

# Má»Ÿ khÃ³a 30 lá»›p cuá»‘i Ä‘á»ƒ há»c sÃ¢u
base_model.trainable = True
for layer in base_model.layers[:-30]:
    layer.trainable = False

# 3. GhÃ©p ná»‘i
inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = data_augmentation(inputs)
x = tf.keras.applications.efficientnet.preprocess_input(x)
x = base_model(x, training=False)
x = layers.GlobalAveragePooling2D()(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.4)(x) # Chá»‘ng há»c váº¹t

# Output layer
outputs = layers.Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.0001))(x)

model = models.Model(inputs, outputs)

# 4. Compile vá»›i Label Smoothing (Chá»‘ng áº£o tÆ°á»Ÿng sá»©c máº¡nh)
model.compile(optimizer=optimizers.Adam(learning_rate=1e-5),
              loss=losses.CategoricalCrossentropy(label_smoothing=0.1),
              metrics=['accuracy'])

model.summary()
print("âœ… Model Ä‘Ã£ lÃªn nÃ²ng! Sáºµn sÃ ng Training!")

# ==========================================
# START TRAINING
# ==========================================
# LÆ°u model tá»‘t nháº¥t
checkpoint = callbacks.ModelCheckpoint("best_model_pro.keras", save_best_only=True, monitor='val_accuracy')
# Äiá»u chá»‰nh tá»‘c Ä‘á»™ há»c
lr_callback = callbacks.LearningRateScheduler(lr_scheduler)

print("\nğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N (CHáº¾ Äá»˜ PRO)...")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[checkpoint, lr_callback],
    class_weight=class_weights_dict # Ãp dá»¥ng trá»ng sá»‘
)
print("ğŸ‰ ÄÃƒ TRAIN XONG!")

# Váº½ biá»ƒu Ä‘á»“
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(len(acc))

plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Äá»™ chÃ­nh xÃ¡c (Accuracy)')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Má»©c Ä‘á»™ lá»—i (Loss)')
plt.show()

# ==========================================
# PHÃ‚N TÃCH CHUYÃŠN SÃ‚U
# ==========================================
print("ğŸ“Š Äang táº¡o ma tráº­n nháº§m láº«n (Confusion Matrix)...")

# Load láº¡i model tá»‘t nháº¥t (Ä‘á»ƒ Ä‘áº£m báº£o khÃ´ng dÃ¹ng cÃ¡i model á»Ÿ epoch cuá»‘i cÃ¹ng náº¿u nÃ³ bá»‹ lá»Ÿm)
model.load_weights("best_model_pro.keras")

y_true = []
y_pred = []

# Dá»± Ä‘oÃ¡n toÃ n bá»™ táº­p validation
for images, labels in val_ds:
    preds = model.predict(images, verbose=0)
    y_true.extend(np.argmax(labels.numpy(), axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

# Váº½ Heatmap
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(16, 14))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Model Dá»± ÄoÃ¡n')
plt.ylabel('Thá»±c Táº¿ (NhÃ£n ÄÃºng)')
plt.title('Báº¢N Äá»’ NHáº¦M LáºªN')
plt.show()

# In bÃ¡o cÃ¡o chi tiáº¿t
print(classification_report(y_true, y_pred, target_names=class_names))