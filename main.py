import cv2
import mediapipe as mp
import numpy as np
import tensorflow as tf
# üëá Hai d√≤ng quan tr·ªçng con ƒëang thi·∫øu ƒë√¢y:
from tensorflow.keras.models import load_model
from tensorflow.keras.applications.efficientnet import preprocess_input
import time
import os

# ==========================================
# C·∫§U H√åNH H·ªÜ TH·ªêNG (CONFIG)
# ==========================================
class Config:

    MODEL_PATH = r"model_efficientnet_b0.keras"

    LABELS = ['A', 'B', 'C', 'D', 'E', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X',
              'Y']
    IMG_SIZE = 224
    CONFIDENCE_THRESHOLD = 0.75  # Ng∆∞·ª°ng t·ª± tin ƒë·ªÉ hi·ªÉn th·ªã m√†u xanh
    SMOOTHING_FACTOR = 0.0  # H·ªá s·ªë l√†m m∆∞·ª£t (0.1 -> 0.9). C√†ng cao c√†ng m∆∞·ª£t nh∆∞ng tr·ªÖ h∆°n x√≠u.
    CAMERA_ID = 0  # ID Camera (th∆∞·ªùng l√† 0 ho·∫∑c 1)
    FRAME_WIDTH = 1280  # ƒê·ªô ph√¢n gi·∫£i HD cho n√©t
    FRAME_HEIGHT = 720


# T·∫Øt c·∫£nh b√°o TensorFlow cho s·∫°ch m√†n h√¨nh console
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


# ==========================================
# MODULE 1: PH√ÅT HI·ªÜN TAY (HAND DETECTOR)
# ==========================================
class HandDetector:
    def __init__(self):
        self.mp_hands = mp.solutions.hands
        # C·∫•u h√¨nh MediaPipe t·ªëi ∆∞u cho t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c
        self.hands = self.mp_hands.Hands(
            model_complexity=1,  # 1: C√¢n b·∫±ng, 0: Nhanh, 2: Ch√≠nh x√°c nh·∫•t
            max_num_hands=1,  # Ch·ªâ b·∫Øt 1 tay ƒë·ªÉ tr√°nh nhi·ªÖu
            min_detection_confidence=0.7,
            min_tracking_confidence=0.6
        )
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles

    def find_hands(self, frame):
        """Nh·∫≠n di·ªán v√† tr·∫£ v·ªÅ landmarks + bounding box"""
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.hands.process(frame_rgb)
        hands_data = []
        h, w, c = frame.shape

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # T√≠nh Bounding Box
                x_vals = [lm.x * w for lm in hand_landmarks.landmark]
                y_vals = [lm.y * h for lm in hand_landmarks.landmark]

                pad = 40  # Padding v·ª´a ƒë·ªß
                bbox = {
                    'x_min': max(0, int(min(x_vals)) - pad),
                    'y_min': max(0, int(min(y_vals)) - pad),
                    'x_max': min(w, int(max(x_vals)) + pad),
                    'y_max': min(h, int(max(y_vals)) + pad)
                }

                hands_data.append({
                    'landmarks': hand_landmarks,
                    'bbox': bbox
                })
        return hands_data, results

    def draw_styled_landmarks(self, frame, results):
        """V·∫Ω x∆∞∆°ng tay ƒë·∫πp m·∫Øt"""
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                self.mp_drawing.draw_landmarks(
                    frame,
                    hand_landmarks,
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style())


# ==========================================
# MODULE 2: PH√ÇN LO·∫†I K√ù HI·ªÜU (SIGN CLASSIFIER)
# ==========================================
class SignClassifier:
    def __init__(self, model_path, labels, img_size):
        print(f"‚è≥ ƒêang n·∫°p AI Model t·ª´: {model_path}...")
        try:
            self.model = load_model(model_path)
            print("‚úÖ AI Model ƒë√£ s·∫µn s√†ng!")
        except Exception as e:
            print(f"‚ùå L·ªói n·∫°p Model: {e}")
            exit()
        self.labels = labels
        self.img_size = img_size
        # Bi·∫øn ƒë·ªÉ l∆∞u x√°c su·∫•t c≈© (cho thu·∫≠t to√°n l√†m m∆∞·ª£t)
        self.prev_probs = None

    def preprocess(self, frame, bbox):
        """C·∫Øt ·∫£nh v√† gi·ªØ nguy√™n gi√° tr·ªã g·ªëc cho Model t·ª± x·ª≠ l√Ω"""
        x_min, y_min = bbox['x_min'], bbox['y_min']
        x_max, y_max = bbox['x_max'], bbox['y_max']

        # Ki·ªÉm tra c·∫Øt ·∫£nh h·ª£p l·ªá
        if x_max - x_min <= 0 or y_max - y_min <= 0:
            return None

        hand_crop = frame[y_min:y_max, x_min:x_max]
        if hand_crop.size == 0: return None

        # 1. Resize v·ªÅ 224x224
        img = cv2.resize(hand_crop, (self.img_size, self.img_size))

        # 2. Chuy·ªÉn BGR sang RGB
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # 3. GI·ªÆ NGUY√äN (Kh√¥ng chia 255, kh√¥ng preprocess)
        # Model EfficientNet con train ƒë√£ c√≥ l·ªõp x·ª≠ l√Ω b√™n trong r·ªìi.
        img_batch = np.expand_dims(img, axis=0)

        return img_batch

    def predict_with_smoothing(self, img_batch):
        """D·ª± ƒëo√°n v√† √°p d·ª•ng thu·∫≠t to√°n l√†m m∆∞·ª£t (Exponential Moving Average)"""
        current_probs = self.model.predict(img_batch, verbose=0)[0]

        if self.prev_probs is None:
            self.prev_probs = current_probs
        else:
            # C√¥ng th·ª©c l√†m m∆∞·ª£t: Probs m·ªõi = alpha * Probs hi·ªán t·∫°i + (1-alpha) * Probs c≈©
            self.prev_probs = (Config.SMOOTHING_FACTOR * self.prev_probs +
                               (1 - Config.SMOOTHING_FACTOR) * current_probs)

        smoothed_probs = self.prev_probs
        idx = np.argmax(smoothed_probs)
        label = self.labels[idx]
        confidence = smoothed_probs[idx]
        return label, confidence


# ==========================================
# MODULE 3: CH∆Ø∆†NG TR√åNH CH√çNH (MAIN APP)
# ==========================================
class SignLanguageApp:
    def __init__(self):
        self.detector = HandDetector()
        self.classifier = SignClassifier(Config.MODEL_PATH, Config.LABELS, Config.IMG_SIZE)
        self.cap = cv2.VideoCapture(Config.CAMERA_ID)
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, Config.FRAME_WIDTH)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, Config.FRAME_HEIGHT)
        self.fps_start_time = 0

    def draw_ui(self, frame, bbox, label, conf, fps):
        """V·∫Ω giao di·ªán chuy√™n nghi·ªáp"""
        # 1. V·∫Ω BBox v√† Nh√£n tr√™n tay
        if bbox:
            color = (0, 255, 0) if conf > Config.CONFIDENCE_THRESHOLD else (0, 165, 255)

            # V·∫Ω khung
            cv2.rectangle(frame, (bbox['x_min'], bbox['y_min']), (bbox['x_max'], bbox['y_max']), color, 2)

            # V·∫Ω n·ªÅn ch·ªØ (Semi-transparent)
            label_text = f"{label} ({conf * 100:.0f}%)"
            (w, h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
            cv2.rectangle(frame, (bbox['x_min'], bbox['y_min'] - 35), (bbox['x_min'] + w + 10, bbox['y_min']), color,
                          -1)

            # Vi·∫øt ch·ªØ
            cv2.putText(frame, label_text, (bbox['x_min'] + 5, bbox['y_min'] - 8),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

        # 2. V·∫Ω FPS v√† th√¥ng tin g√≥c m√†n h√¨nh
        cv2.rectangle(frame, (0, 0), (250, 50), (0, 0, 0), -1)  # N·ªÅn ƒëen g√≥c
        cv2.putText(frame, f"FPS: {int(fps)} | 'Q' to Exit", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

    def run(self):
        print("üé• ƒêang kh·ªüi ƒë·ªông Camera... Vui l√≤ng ch·ªù!")
        while self.cap.isOpened():
            success, frame = self.cap.read()
            if not success: break

            frame = cv2.flip(frame, 1)  # L·∫≠t g∆∞∆°ng
            hands_data, results = self.detector.find_hands(frame)

            label = ""
            conf = 0.0
            bbox = None

            # Ch·ªâ x·ª≠ l√Ω n·∫øu ph√°t hi·ªán tay
            if hands_data:
                hand = hands_data[0]  # L·∫•y tay ƒë·∫ßu ti√™n
                bbox = hand['bbox']

                # X·ª≠ l√Ω ·∫£nh
                img_batch = self.classifier.preprocess(frame, bbox)

                if img_batch is not None:
                    # D·ª± ƒëo√°n v·ªõi l√†m m∆∞·ª£t
                    label, conf = self.classifier.predict_with_smoothing(img_batch)

                # V·∫Ω x∆∞∆°ng (T√πy ch·ªçn, comment d√≤ng d∆∞·ªõi n·∫øu mu·ªën t·∫Øt x∆∞∆°ng)
                #self.detector.draw_styled_landmarks(frame, results)

            # T√≠nh FPS
            fps_end_time = time.time()
            time_diff = fps_end_time - self.fps_start_time
            fps = 1 / time_diff if time_diff > 0 else 0
            self.fps_start_time = fps_end_time

            # V·∫Ω giao di·ªán
            self.draw_ui(frame, bbox, label, conf, fps)

            cv2.imshow('Sign Language AI - Pro Version', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        self.cap.release()
        cv2.destroyAllWindows()


# ==========================================
# CH·∫†Y CH∆Ø∆†NG TR√åNH
# ==========================================
if __name__ == "__main__":
    app = SignLanguageApp()
    app.run()